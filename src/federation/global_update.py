import numpy as np
import torch.multiprocessing as mp
import gym

from src.utils.fed_utils import init_model, reward_fn_from_string, apply_wrappers
from src.federation.aggregation import init_aggregator

from sinergym_extend.utils.evaluation import evaluate_policy
from sinergym_extend.utils.logging import configure

from stable_baselines3.common.policies import BasePolicy

from typing import Dict, Any, Type, Union, List, Optional

from copy import deepcopy

import csv
import os

class GlobalUpdate(mp.Process):
    """ Class for a GlobalUpdate process object, which organizes the global update during federated learning by
    retrieving the local weight differences of each client after local updates and computing the updated weights 
    of the global agent. Also performs evaluation of the global agent if so desired.
    
    Args:
        algo (str) : The RL algorithm to use for the evaluation model.
        global_policy (Type[BasePolicy]) : The global policy object.
        eval_env_name (str) : Name of the environment used for evaluation.
        global_eval_path (Union[str, os.PathLike]) : Path to directory storing evaluation results.
        aggregation_technique (str) : The federated algorithm used for training (available techniques: FedAvg, FedAvgM and FedAdam).
        reward_name (str) : Name of reward function used (available: linear, exponential, gaustrap).
        global_learning_rate (float) : The learning rate of the global update.
        masking_threshold (float) : The threshold value (tau) of the gradient masking procedure.
        server_momentum (float) : The momentum parameter (beta) used by FedAvgM.
        betas (List[float]) : The beta parameter values used in the adaptive federated algo FedADAM.
        adapt_degree (float) : The adaptive degree parameter of adaptive federated algos.
        delta_queue (mp.Queue) : Process shared Queue object for communicating the local differences in policy network 
                                parameters after clients' local gradient updates to the global update process.
        size_queue (mp.Queue) : Process shared Queue object for communicating the size of the clients' local replay buffers
                                to the global update process.
        barrier (mp.Barrier) : Barrier object for synchronizing the client and global update processes.
        num_clients (int) : Number of participating client processes.
        episodes (int) : Total number of episodes to train for.
        num_rounds_per_episode (int) : How many communication rounds to perform over one episode.
        timesteps_per_episode (int) : Total number of environment steps to take over one episode.
        max_ep_data_store_num (int): How many episode logs generated by Sinergym to store. Stores the max_ep_data_store_num most 
                                    recent episodes. The logs require a significant amount of memory, so it is recommended to keep 
                                    it low when running several experiments.
        wrapper_config (Dict[str, bool]) : Dictionary of configuration parameters for wrapper functions.                        
        eval_config (Dict[str, Any]): Dictionary of evaluation configuration parameters.
        seed (Optional[int]) : Random seed value of the global evaluation model.

    """

    def __init__(self, 
                algo: str,
                global_policy: Type[BasePolicy],
                eval_env_name: str,
                global_eval_path: Union[str, os.PathLike],
                aggregation_technique: str,
                reward_name: str,
                global_learning_rate: float,
                masking_threshold: float,
                server_momentum: float,
                betas: List[float],
                adapt_degree: float,
                delta_queue: mp.Queue,
                size_queue: mp.Queue,
                barrier: mp.Barrier,
                num_clients: int,
                episodes: int,
                num_rounds_per_episode: int, 
                timesteps_per_episode: int,
                max_ep_data_store_num: int,
                wrapper_config: Dict[str, bool],
                eval_config: Dict[str, Any],
                seed: Optional[int]=None,
                ):

        super().__init__()

        self.algo = algo
        self.global_policy = global_policy
        self.eval_env_name = eval_env_name
        self.__eval_model = None
        self.eval_env = None
        self.eval_path = os.path.join(global_eval_path, "evaluation")
        self.best_model_save_path = os.path.join(global_eval_path, "best_model")

        self.aggregator = init_aggregator(aggregation_technique,
                                        global_learning_rate = global_learning_rate,
                                        masking_threshold = masking_threshold,
                                        policy_dict = deepcopy(self.global_policy.state_dict()),
                                        momentum = server_momentum,
                                        betas = betas,
                                        total_rounds = episodes * num_rounds_per_episode,
                                        # weight_decay = weight_decay,
                                        adapt_degree = adapt_degree)

        self.reward_fn = reward_fn_from_string(reward_name)

        self.delta_queue = delta_queue
        self.size_queue = size_queue
        self.barrier = barrier

        self.num_clients = num_clients
        self.episodes = episodes
        self.max_ep_data_store_num = max_ep_data_store_num
        self.num_rounds_per_episode = num_rounds_per_episode
        self.timesteps_per_episode = timesteps_per_episode
        self.curr_episode = 1

        self.evaluate = eval_config["eval"]
        self.n_eval_episodes = eval_config["eval_length"]

        self.wrapper_config = wrapper_config
        self.seed = seed

        if self.evaluate:
            self.evaluations_timesteps = []
            self.evaluations_results = []
            self.evaluations_length = []
            self.evaluations_power_consumption = []
            self.evaluations_comfort_violation = []
            self.evaluations_comfort_reward = []
            self.evaluations_power_penalty = []
            self.evaluation_metrics = {}

            self.best_mean_reward = -np.inf

            eval_summary_header_list = [
            'episode',
            'mean_rewards',
            'std_rewards',
            'mean_cumulative_power_consumption',
            'std_cumulative_power_consumption',
            'mean_comfort_violation (%)',
            'mean_comfort_reward',
            'mean_power_penalty',
            ]
            self.eval_summary_header = ''
            for element_header in eval_summary_header_list:
                self.eval_summary_header += element_header + ','
            self.eval_summary_header = self.eval_summary_header[:-1] + '\n'

            self.log_summary_file = self.eval_path + '/eval_summary.csv'


    @property
    def eval_policy(self) -> Type[BasePolicy]:
        """Getter function for the policy of the RL model used in evaluation."""
        return self.__eval_model.policy

    @eval_policy.setter
    def eval_policy(self, policy) -> None:
        """Policy setter for passing globally aggregated policy parameters to evaluation model."""
        params = {'policy': policy.state_dict()}
        self.__eval_model.set_parameters(params, exact_match=False)

    def global_eval(self) -> None:
        """ Evaluates the performance of the global agent and logs the results. """

        episodes_data = evaluate_policy(self.__eval_model,
                                        self.eval_env,
                                        n_eval_episodes=self.n_eval_episodes,
                                        render=False,
                                        deterministic=True,
                                        callback=None,
                                        )

        # Get the cumulative number of timesteps taken so far for logging purposes
        num_timesteps = self.curr_episode * self.timesteps_per_episode

        mean_reward, std_reward = np.mean(episodes_data['episodes_rewards']), np.std(episodes_data['episodes_rewards'])
        mean_ep_length = np.mean(episodes_data['episodes_lengths'])

        self.evaluation_metrics['episode'] = self.curr_episode
        self.evaluation_metrics['mean_rewards'] = mean_reward
        self.evaluation_metrics['std_rewards'] = std_reward
        self.evaluation_metrics['mean_ep_length'] = mean_ep_length
        self.evaluation_metrics['mean_power_consumption'] = np.mean(episodes_data['episodes_powers'])
        self.evaluation_metrics['std_power_consumption'] = np.std(episodes_data['episodes_powers'])
        self.evaluation_metrics['comfort_violation(%)'] = np.mean(episodes_data['episodes_comfort_violations'])
        self.evaluation_metrics['comfort_reward'] = np.mean(episodes_data['episodes_comfort_rewards'])
        self.evaluation_metrics['power_penalty'] = np.mean(episodes_data['episodes_power_penalties'])

        # Add to current Logger
        for key, metric in self.evaluation_metrics.items():
            self.logger.record('global_eval/' + key, metric)
        self.logger.dump(step=num_timesteps)

        # Create CSV file with header if it's required for eval_summary.csv
        if not os.path.isfile(self.log_summary_file):
            with open(self.log_summary_file, 'a', newline='\n') as file_obj:
                file_obj.write(self.eval_summary_header)

        # building episode row
        row_contents = [
            self.curr_episode,
            self.evaluation_metrics['mean_rewards'],
            self.evaluation_metrics['std_rewards'],
            self.evaluation_metrics['mean_power_consumption'],
            self.evaluation_metrics['std_power_consumption'],
            self.evaluation_metrics['comfort_violation(%)'],
            self.evaluation_metrics['comfort_reward'],
            self.evaluation_metrics['power_penalty'],
        ]

        with open(self.log_summary_file, 'a+', newline='') as file_obj:
            # Create a writer object from csv module
            csv_writer = csv.writer(file_obj)
            # Add contents of list as last row in the csv file
            csv_writer.writerow(row_contents)

        if mean_reward > self.best_mean_reward:
            print("New best mean reward! \n")
            self.best_mean_reward = mean_reward
            if self.best_model_save_path is not None:
                self.__eval_model.save(os.path.join(self.best_model_save_path, 'model.zip'))
                

    def run(self) -> None:
        """ Executes the global update process of federated training. """

        if self.evaluate:
            self.eval_env = gym.make(self.eval_env_name, 
                                    reward=self.reward_fn,  
                                    max_ep_data_store_num=self.max_ep_data_store_num + 1, # Add 1 because Sinergym always needlessly generates an extra subrun folder at end of training. 
                                                                                          # This way we ensure that the folders of actual training episodes aren't deleted.
                                    experiment_path=self.eval_path)

            self.eval_env = apply_wrappers(self.eval_env, self.wrapper_config)

            self.__eval_model = init_model(self.algo, self.eval_env, self.seed)

            self.logger = configure(self.eval_path, ["stdout", "tensorboard"], max_length=72)

        client_deltas = []
        client_sizes = []
        for ep in range(self.episodes):
            for r in range(self.num_rounds_per_episode):
                
                # Wait for every client to complete their local updates
                self.barrier.wait()

                # Retrieve the parameter deltas and replay buffer sizes of participating clients
                while len(client_deltas) < self.num_clients:
                        d = self.delta_queue.get()
                        s = self.size_queue.get()

                        client_deltas.append(d)
                        client_sizes.append(s)
                
                selected_total_size = sum(client_sizes)

                # calculate aggregation coefficients of client weights
                mixing_coefficients = [size / selected_total_size for size in client_sizes]

                # aggregate each clients' updated model parameters and update the global model
                global_weights = self.aggregator(self.global_policy.state_dict(), 
                                                client_deltas,
                                                mixing_coefficients)
                self.global_policy.load_state_dict(global_weights)

                # Reset delta and size lists
                client_deltas = []
                client_sizes = []

                # Synchronize client processes with the completion of global update 
                self.barrier.wait()
            
            if self.evaluate:
                self.eval_policy = self.global_policy
                self.global_eval()

            self.curr_episode += 1
